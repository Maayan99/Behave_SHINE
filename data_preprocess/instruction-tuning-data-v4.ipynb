{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuyewei/miniconda3/envs/metalora/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(data, save_path):\n",
    "    import json, os\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        for item in data:\n",
    "            id = item['id']\n",
    "            context = item['context']\n",
    "            conversations = []\n",
    "            for qa in item['qa_pairs']:\n",
    "                question = qa['question']\n",
    "                answer = qa['answer']\n",
    "                conversations.append({\"role\": \"user\", \"content\": question})\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            f.write(json.dumps({\"id\": id, \"context\": context, \"conversations\": conversations}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COQA: 7199, context len: 1357.6557855257674\n",
      "Counter({20: 2452, 10: 1070, 11: 463, 12: 410, 13: 337, 15: 327, 19: 304, 18: 296, 14: 283, 17: 233, 16: 209, 21: 148, 1: 119, 2: 91, 6: 71, 3: 65, 5: 54, 9: 54, 7: 51, 4: 50, 8: 48, 22: 33, 23: 16, 27: 3, 24: 3, 36: 2, 28: 2, 25: 2, 26: 1, 30: 1, 29: 1})\n"
     ]
    }
   ],
   "source": [
    "# COQA ('stanfordnlp/coqa')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('stanfordnlp/coqa')\n",
    "data = list(data['train'])\n",
    "# iterate all items in the dataset\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "item_list = []\n",
    "for i, item in enumerate(data):\n",
    "    context = item['story']\n",
    "    questions = item['questions']\n",
    "    answers = item['answers'][\"input_text\"]\n",
    "    item_list.append({\n",
    "        'context': context,\n",
    "        'qa_pairs': [{'question': q, 'answer': a} for q, a in zip(questions, answers)]\n",
    "    })\n",
    "\n",
    "# randomly merge five of each to one item\n",
    "num_context = 8\n",
    "num_merge = 1\n",
    "for i in range(0, len(item_list), num_merge):\n",
    "    if i + num_merge > len(item_list):\n",
    "        break\n",
    "    context_list = [item_list[j]['context'] for j in range(i, i + num_merge)]\n",
    "    context_list += [item['context'] for item in random.sample(item_list, random.randint(0, num_context))]\n",
    "    random.shuffle(context_list)\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    if len(context.split()) > 4096:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs_list = [item_list[j]['qa_pairs'] for j in range(i, i + num_merge)]\n",
    "    random.shuffle(qa_pairs_list)\n",
    "    qa_pairs = sum(qa_pairs_list, [])\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"coqa.{len(output_list):04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "\n",
    "print(f\"COQA: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/coqa.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# # output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP: 5541, context len: 1067.5722793719544\n",
      "Counter({7: 306, 5: 292, 8: 276, 9: 270, 3: 266, 4: 265, 6: 264, 11: 259, 13: 249, 12: 230, 14: 223, 15: 218, 2: 205, 17: 189, 16: 181, 10: 172, 1: 164, 18: 153, 19: 133, 20: 122, 21: 110, 22: 107, 23: 105, 24: 86, 25: 80, 26: 69, 27: 61, 28: 51, 31: 43, 29: 41, 34: 38, 32: 36, 30: 30, 35: 29, 37: 26, 38: 25, 39: 23, 41: 19, 33: 19, 42: 18, 40: 17, 36: 17, 43: 15, 44: 13, 46: 8, 47: 7, 45: 6, 51: 2, 54: 1, 49: 1, 48: 1})\n"
     ]
    }
   ],
   "source": [
    "# DROP ('ucinlp/drop')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('ucinlp/drop')\n",
    "data = list(data['train'])\n",
    "random.seed(0)\n",
    "\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['passage']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "\n",
    "\n",
    "item_list = []\n",
    "for i, context in enumerate(context_to_item):\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']\n",
    "        answer = item['answers_spans']['spans'][0]\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    item_list.append({\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs,\n",
    "    })\n",
    "\n",
    "\n",
    "num_context = 8\n",
    "num_merge = 1\n",
    "for i in range(0, len(item_list), num_merge):\n",
    "    if i + num_merge > len(item_list):\n",
    "        break\n",
    "    context_list = [item_list[j]['context'] for j in range(i, i + num_merge)]\n",
    "    context_list += [item['context'] for item in random.sample(item_list, random.randint(0, num_context))]\n",
    "    random.shuffle(context_list)\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    if len(context.split()) > 4096:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs_list = [item_list[j]['qa_pairs'] for j in range(i, i + num_merge)]\n",
    "    random.shuffle(qa_pairs_list)\n",
    "    qa_pairs = sum(qa_pairs_list, [])\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"drop.{len(output_list):04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "\n",
    "print(f\"DROP: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/drop.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 24/24 [11:36<00:00, 29.01s/files]  \n",
      "Generating train split: 100%|██████████| 32747/32747 [00:27<00:00, 1178.07 examples/s]\n",
      "Generating test split: 100%|██████████| 10557/10557 [00:08<00:00, 1208.98 examples/s]\n",
      "Generating validation split: 100%|██████████| 3461/3461 [00:02<00:00, 1408.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narrativeqa: 1102, context len: 577.0644283121597\n",
      "Counter({30: 537, 29: 325, 28: 131, 27: 47, 26: 23, 40: 11, 38: 4, 37: 3, 25: 3, 31: 3, 32: 3, 20: 2, 39: 2, 36: 1, 33: 1, 24: 1, 45: 1, 42: 1, 48: 1, 35: 1, 23: 1})\n"
     ]
    }
   ],
   "source": [
    "# narrativeqa ('deepmind/narrativeqa')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('deepmind/narrativeqa')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['document']['summary']['text']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']['text']\n",
    "        answer = item['answers'][0]['text']\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"narrativeqa.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"narrativeqa: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/narrativeqa.jsonl')\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1000/1000 [00:00<00:00, 45665.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmedqa: 1000, context len: 200.207\n",
      "Counter({1: 1000})\n"
     ]
    }
   ],
   "source": [
    "# PubMed ('qiaojin/PubMedQA')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('qiaojin/PubMedQA', 'pqa_labeled')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = \"\\n\".join(item['context']['contexts'])\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    assert len(context_to_item[context]) == 1\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']\n",
    "        answer = f'{item[\"final_decision\"].capitalize()}. {item[\"long_answer\"]}'\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    if sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"pubmedqa.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"pubmedqa: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/pubmedqa.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 10246/10246 [00:00<00:00, 173878.09 examples/s]\n",
      "Generating validation split: 100%|██████████| 2164/2164 [00:00<00:00, 123605.48 examples/s]\n",
      "Generating challenge split: 100%|██████████| 556/556 [00:00<00:00, 65293.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quail: 560, context len: 332.6714285714286\n",
      "Counter({16: 522, 17: 14, 18: 7, 23: 5, 19: 4, 24: 3, 26: 2, 22: 2, 20: 1})\n"
     ]
    }
   ],
   "source": [
    "# quail ('textmachinelab/quail')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('textmachinelab/quail')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['context']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        question = item['question']\n",
    "        answer = item['answers'][item['correct_answer_id']]\n",
    "        if answer == \"not enough information\":\n",
    "            continue\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    if len(qa_pairs) == 0:\n",
    "        continue\n",
    "    if sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"quail.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"quail: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/quail.jsonl')\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quail ('rajpurkar/squad_v2')\n",
    "# output_list = []\n",
    "# data = datasets.load_dataset('rajpurkar/squad_v2')\n",
    "# data = list(data['train'])\n",
    "# context_to_item = {}\n",
    "# for item in data:\n",
    "#     context = item['context']\n",
    "#     if context not in context_to_item:\n",
    "#         context_to_item[context] = []\n",
    "#     context_to_item[context].append(item)\n",
    "# for i, context in enumerate(context_to_item):\n",
    "#     if len(context.split()) > 1024:\n",
    "#         print(f\"skip context len: {len(context.split())}\")\n",
    "#         continue\n",
    "#     qa_pairs = []\n",
    "#     for item in context_to_item[context]:\n",
    "#         if len(item['answers'][\"text\"]) == 0:\n",
    "#             continue\n",
    "#         question = item['question']\n",
    "#         answer = item['answers'][\"text\"][0]\n",
    "#         qa_pairs.append({'question': question, 'answer': answer})\n",
    "#     if len(qa_pairs) == 0:\n",
    "#         continue\n",
    "#     output_list.append({\n",
    "#         \"id\": f\"squad_v2.{i:04d}\",\n",
    "#         'context': context,\n",
    "#         'qa_pairs': qa_pairs\n",
    "#     })\n",
    "# print(f\"squad_v2: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "# print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "# data_all_list.extend(output_list)\n",
    "# # output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 100%|██████████| 10047/10047 [00:00<00:00, 35170.65 examples/s]\n",
      "Generating train split: 100%|██████████| 82326/82326 [00:02<00:00, 33939.22 examples/s]\n",
      "Generating test split: 100%|██████████| 9650/9650 [00:00<00:00, 35106.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msmarco: 19331, context len: 1441.1601055299777\n",
      "Counter({4: 17541, 5: 1463, 6: 232, 7: 52, 8: 20, 9: 9, 10: 7, 11: 5, 12: 2})\n"
     ]
    }
   ],
   "source": [
    "# ms marco ('microsoft/ms_marco')\n",
    "output_list = []\n",
    "data = datasets.load_dataset('microsoft/ms_marco', 'v1.1')\n",
    "data = list(data['train'])\n",
    "random.seed(0)\n",
    "\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    passages = item['passages']\n",
    "    if sum(passages['is_selected']) == 0:\n",
    "        continue\n",
    "    is_selected_idx = passages['is_selected'].index(1)\n",
    "    assert isinstance(is_selected_idx, int)\n",
    "    context = passages['passage_text'][is_selected_idx]\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "item_list = []\n",
    "for i, context in enumerate(context_to_item):\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        if len(item['answers']) == 0:\n",
    "            continue\n",
    "        question = item['query']\n",
    "        answer = item['answers'][0]\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    if len(qa_pairs) == 0:\n",
    "        continue\n",
    "    item_list.append({\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs,\n",
    "    })\n",
    "# randomly merge five of each to one item\n",
    "num_context = 32\n",
    "num_merge = 4\n",
    "for i in range(0, len(item_list), num_merge):\n",
    "    if i + num_merge > len(item_list):\n",
    "        break\n",
    "    context_list = [item_list[j]['context'] for j in range(i, i + num_merge)]\n",
    "    context_list += [item['context'] for item in random.sample(item_list, random.randint(0, num_context))]\n",
    "    random.shuffle(context_list)\n",
    "    context = \"\\n\\n\".join(context_list)\n",
    "    if len(context.split()) > 4096:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs_list = [item_list[j]['qa_pairs'] for j in range(i, i + num_merge)]\n",
    "    random.shuffle(qa_pairs_list)\n",
    "    qa_pairs = sum(qa_pairs_list, [])\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        # continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"msmarco.{len(output_list):04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "\n",
    "print(f\"msmarco: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/msmarco.jsonl')\n",
    "\n",
    "# data_all_list.extend(output_list)\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 241564/241564 [00:01<00:00, 197568.10 examples/s]\n",
      "Generating test split: 100%|██████████| 18146/18146 [00:00<00:00, 190239.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwc: 16382, context len: 354.6154926138445\n",
      "Counter({10: 3951, 11: 3348, 14: 2853, 13: 2582, 12: 1531, 9: 803, 15: 785, 8: 341, 7: 154, 6: 32, 5: 2})\n"
     ]
    }
   ],
   "source": [
    "# PwC\n",
    "output_list = []\n",
    "data = datasets.load_dataset('sggetao/PwC')\n",
    "data = list(data['train'])\n",
    "context_to_item = {}\n",
    "for item in data:\n",
    "    context = item['input']\n",
    "    if context not in context_to_item:\n",
    "        context_to_item[context] = []\n",
    "    context_to_item[context].append(item)\n",
    "for i, context in enumerate(context_to_item):\n",
    "    if len(context.split()) > 2048:\n",
    "        print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    qa_pairs = []\n",
    "    for item in context_to_item[context]:\n",
    "        if len(item['answer'].split()) > 128 or len(item['prompt'].split()) > 128:\n",
    "            continue\n",
    "        question = item['prompt']\n",
    "        answer = item['answer']\n",
    "        qa_pairs.append({'question': question, 'answer': answer})\n",
    "    # randomly sample 5 qa pairs for each context\n",
    "    # qa_pairs = qa_pairs[:8]\n",
    "    while sum([len(qa[\"question\"].split()) for qa in qa_pairs]) + sum([len(qa[\"answer\"].split()) for qa in qa_pairs]) > 512:\n",
    "        qa_pairs = qa_pairs[:-1]\n",
    "        # print(f\"q eln: {[len(qa['question'].split()) for qa in qa_pairs]}, a len: {[len(qa['answer'].split()) for qa in qa_pairs]}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"pwc.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': qa_pairs\n",
    "    })\n",
    "print(f\"pwc: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/pwc.jsonl')\n",
    "# output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/metaicl/hr_to_lr.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m output_list = []\n\u001b[32m      6\u001b[39m random.seed(\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../data/metaicl/hr_to_lr.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      9\u001b[39m     config = json.load(f)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tqdm(config[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/metalora/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/metaicl/hr_to_lr.json'"
     ]
    }
   ],
   "source": [
    "import json, torch, transformers, random\n",
    "from tqdm import tqdm\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('../models/Qwen3-8B/')\n",
    "\n",
    "output_list = []\n",
    "random.seed(0)\n",
    "\n",
    "with open(\"../../data/metaicl/hr_to_lr.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "for task in tqdm(config[\"train\"]):\n",
    "    dp = torch.load(f'../../data/metaicl/llama/{task}/{task}_16384_100_train.jsonl')\n",
    "    k_shot = 64\n",
    "    k_test = 4\n",
    "    max_num = 200\n",
    "    n = len(dp['input'])\n",
    "\n",
    "\n",
    "    input_text = [tokenizer.decode(dp['input'][i]) for i in range(n)]\n",
    "    output_text = [tokenizer.decode(dp['output'][i]) for i in range(n)]\n",
    "    avg_input_len = sum([len(x.split()) for x in input_text]) / n\n",
    "    k_shot = min(k_shot, 2048 // int(avg_input_len))\n",
    "    k_test = min(k_test, 512 // int(avg_input_len))\n",
    "    \n",
    "    if k_shot == 0 or k_test == 0:\n",
    "        print(f\"skip {task} due to avg_input_len: {avg_input_len}\")\n",
    "        continue\n",
    "\n",
    "    # reduce the number of examples\n",
    "    n = min(n, max_num * (k_shot + k_test))\n",
    "    index = random.sample(range(len(input_text)), n)\n",
    "    input_text = [input_text[i] for i in index]\n",
    "    output_text = [output_text[i] for i in index]\n",
    "\n",
    "    # first k are demonstrations, the rest are test case\n",
    "    for i in range(0, n, k_shot + k_test):\n",
    "        if i + k_shot + k_test > n:\n",
    "            break\n",
    "        context = \"\\n\\n\".join([f\"Input: {input_text[j]}\\nOutput: {output_text[j]}\" for j in range(i, i + k_shot)])\n",
    "        # test_input_text = f\"Input: {input_text[i + k]}\\nOutput: \"\n",
    "        # test_output_text = f\"{output_text[i + k]}\"\n",
    "        output_list.append({\n",
    "            \"id\": f\"metaicl.{len(output_list):04d}.{task}.{i // (k_shot + k_test)}\",\n",
    "            'context': context,\n",
    "            'qa_pairs': [\n",
    "                {'question': f\"Input: {input_text[j]}\\nOutput:\", 'answer': output_text[j]} for j in range(i + k_shot, i + k_shot + k_test)\n",
    "            ],\n",
    "        })\n",
    "    print(task, n, avg_input_len, k_shot, k_test, len(output_list))\n",
    "print(f\"metaicl: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/metaicl.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 12000/12000 [00:06<00:00, 1931.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONGALPACA: 4746, context len: 4964.772439949431\n",
      "Counter({2: 4746})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Long Alpaca\n",
    "# data_path = \"../../data/long-llm/longalpaca/train.json\"\n",
    "# data = datasets.load_dataset('json', data_files=data_path)\n",
    "data = datasets.load_dataset('Yukang/LongAlpaca-12k')\n",
    "data = list(data[\"train\"])\n",
    "output_list = []\n",
    "for i, item in enumerate(data):\n",
    "    first_message = item[\"instruction\"]\n",
    "    second_message = item[\"output\"]\n",
    "    match_success = False\n",
    "    try:\n",
    "        result = re.search(r\"^(.*)Now the (.*?) ends\\.(.*?)$\", first_message, re.DOTALL)\n",
    "        assert result is not None, f\"id: {i}\"\n",
    "        instruction = result.group(3).strip()\n",
    "        context = first_message[:first_message.index(result.group(3))].strip()\n",
    "        response = second_message\n",
    "        match_success = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if not match_success:\n",
    "        continue\n",
    "\n",
    "    # if the lenth of the context exeed 8192, skip\n",
    "    if len(context.split()) > 6144:\n",
    "        continue    \n",
    "\n",
    "    output_list.append({\n",
    "        \"id\": f\"longalpaca.{i:05d}\",\n",
    "        \"context\": context,\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "    })\n",
    "# data_all_list.extend(output_list)\n",
    "print(f\"LONGALPACA: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"conversations\"]) for item in output_list]))\n",
    "# save output_list to v2/longalpaca.jsonl\n",
    "with open(\"v4/longalpaca.jsonl\", \"w\") as f:\n",
    "    for item in output_list:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 9600/9600 [00:02<00:00, 3947.14 examples/s]\n",
      "Generating validation split: 100%|██████████| 1484/1484 [00:00<00:00, 4903.18 examples/s]\n",
      "Generating test split: 100%|██████████| 1431/1431 [00:00<00:00, 5012.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booksum: 5802, context len: 2058.730782488797\n",
      "Counter({1: 5802})\n"
     ]
    }
   ],
   "source": [
    "# booksum\n",
    "output_list = []\n",
    "data = datasets.load_dataset('kmfoda/booksum')\n",
    "data = list(data['train'])\n",
    "# iterate all items in the dataset\n",
    "for i, item in enumerate(data):\n",
    "    assert item is not None\n",
    "    book_id = item[\"book_id\"].split(\".\")[0]\n",
    "    summary_id = item[\"summary_id\"]\n",
    "    context = f\"{book_id}, {summary_id}:\\n\\n{item['chapter']}\"\n",
    "    question = f\"Summarize {book_id}, {summary_id}\"\n",
    "    answer = item['summary_text']\n",
    "    if len(context.split()) > 4096:\n",
    "        # print(f\"skip context len: {len(context.split())}\")\n",
    "        continue\n",
    "    if len(question.split()) + len(answer.split()) > 512:\n",
    "        # print(f\"q len: {len(question.split())}, a len: {len(answer.split())}\")\n",
    "        continue\n",
    "    output_list.append({\n",
    "        \"id\": f\"booksum.{i:04d}\",\n",
    "        'context': context,\n",
    "        'qa_pairs': [{'question': question, 'answer': answer}]\n",
    "    })\n",
    "print(f\"booksum: {len(output_list)}, context len: {sum([len(x['context'].split()) for x in output_list]) / len(output_list)}\")\n",
    "print(collections.Counter([len(item[\"qa_pairs\"]) for item in output_list]))\n",
    "save(output_list, 'v4/booksum.jsonl')\n",
    "# data_all_list.extend(output_list)\n",
    "# # output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sft-v4-qa.jsonl: 34733\n",
      "sft-v4-ift.jsonl: 45281\n"
     ]
    }
   ],
   "source": [
    "# concatenate all \"v4/*.jsonl\" to \"sft-v4.jsonl\"\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "config = {\n",
    "    \"sft-v4-qa.jsonl\": [\"coqa\", \"drop\", \"narrativeqa\", \"pubmedqa\", \"quail\", \"msmarco\"],\n",
    "    \"sft-v4-ift.jsonl\": [\"coqa\", \"drop\", \"narrativeqa\", \"pubmedqa\", \"quail\", \"msmarco\", \"booksum\", \"longalpaca\"],\n",
    "    # \"sft-v4.jsonl\": [\"coqa\", \"drop\", \"narrativeqa\", \"pubmedqa\", \"quail\", \"msmarco\", \"booksum\", \"longalpaca\", \"metaicl\"],\n",
    "}\n",
    "\n",
    "for output_path, dataset_names in config.items():\n",
    "    output_list = []\n",
    "    for dataset_name in dataset_names:\n",
    "        with open(f\"v4/{dataset_name}.jsonl\", \"r\") as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "        output_list.extend(data)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for item in output_list:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "    print(f\"{output_path}: {len(output_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_docs</th>\n",
       "      <th>n_instructions</th>\n",
       "      <th>n_responses</th>\n",
       "      <th>doc_len</th>\n",
       "      <th>instruction_len_mean</th>\n",
       "      <th>response_len_mean</th>\n",
       "      <th>instruction_len_max</th>\n",
       "      <th>response_len_max</th>\n",
       "      <th>instruction_len_min</th>\n",
       "      <th>response_len_min</th>\n",
       "      <th>instruction_len_sum</th>\n",
       "      <th>response_len_sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>booksum</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2058.730782</td>\n",
       "      <td>6.656670</td>\n",
       "      <td>228.837125</td>\n",
       "      <td>6.656670</td>\n",
       "      <td>228.837125</td>\n",
       "      <td>6.656670</td>\n",
       "      <td>228.837125</td>\n",
       "      <td>6.656670</td>\n",
       "      <td>228.837125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coqa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>15.091957</td>\n",
       "      <td>15.091957</td>\n",
       "      <td>1357.655786</td>\n",
       "      <td>5.529221</td>\n",
       "      <td>2.710168</td>\n",
       "      <td>9.621475</td>\n",
       "      <td>8.134741</td>\n",
       "      <td>2.221003</td>\n",
       "      <td>1.068482</td>\n",
       "      <td>82.389637</td>\n",
       "      <td>40.053341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drop</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.469951</td>\n",
       "      <td>13.469951</td>\n",
       "      <td>1067.572279</td>\n",
       "      <td>10.901193</td>\n",
       "      <td>1.398596</td>\n",
       "      <td>16.408591</td>\n",
       "      <td>2.797690</td>\n",
       "      <td>6.790652</td>\n",
       "      <td>1.016604</td>\n",
       "      <td>145.391807</td>\n",
       "      <td>18.761956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longalpaca</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4964.772440</td>\n",
       "      <td>16.827223</td>\n",
       "      <td>210.755162</td>\n",
       "      <td>16.827223</td>\n",
       "      <td>210.755162</td>\n",
       "      <td>16.827223</td>\n",
       "      <td>210.755162</td>\n",
       "      <td>16.827223</td>\n",
       "      <td>210.755162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msmarco</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.119032</td>\n",
       "      <td>4.119032</td>\n",
       "      <td>1441.160106</td>\n",
       "      <td>6.024079</td>\n",
       "      <td>14.095591</td>\n",
       "      <td>8.639698</td>\n",
       "      <td>30.951218</td>\n",
       "      <td>3.892039</td>\n",
       "      <td>2.684962</td>\n",
       "      <td>24.830480</td>\n",
       "      <td>57.934820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>narrativeqa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.438294</td>\n",
       "      <td>29.438294</td>\n",
       "      <td>577.064428</td>\n",
       "      <td>8.580034</td>\n",
       "      <td>4.555363</td>\n",
       "      <td>16.157895</td>\n",
       "      <td>13.985481</td>\n",
       "      <td>3.803993</td>\n",
       "      <td>1.027223</td>\n",
       "      <td>252.284029</td>\n",
       "      <td>133.589837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmedqa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.207000</td>\n",
       "      <td>12.913000</td>\n",
       "      <td>40.660000</td>\n",
       "      <td>12.913000</td>\n",
       "      <td>40.660000</td>\n",
       "      <td>12.913000</td>\n",
       "      <td>40.660000</td>\n",
       "      <td>12.913000</td>\n",
       "      <td>40.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quail</th>\n",
       "      <td>1.0</td>\n",
       "      <td>16.241071</td>\n",
       "      <td>16.241071</td>\n",
       "      <td>332.671429</td>\n",
       "      <td>8.669452</td>\n",
       "      <td>4.925956</td>\n",
       "      <td>14.942857</td>\n",
       "      <td>11.891071</td>\n",
       "      <td>4.260714</td>\n",
       "      <td>1.194643</td>\n",
       "      <td>140.471429</td>\n",
       "      <td>79.723214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n_docs  n_instructions  n_responses      doc_len  \\\n",
       "dataset                                                         \n",
       "booksum         1.0        1.000000     1.000000  2058.730782   \n",
       "coqa            1.0       15.091957    15.091957  1357.655786   \n",
       "drop            1.0       13.469951    13.469951  1067.572279   \n",
       "longalpaca      1.0        1.000000     1.000000  4964.772440   \n",
       "msmarco         1.0        4.119032     4.119032  1441.160106   \n",
       "narrativeqa     1.0       29.438294    29.438294   577.064428   \n",
       "pubmedqa        1.0        1.000000     1.000000   200.207000   \n",
       "quail           1.0       16.241071    16.241071   332.671429   \n",
       "\n",
       "             instruction_len_mean  response_len_mean  instruction_len_max  \\\n",
       "dataset                                                                     \n",
       "booksum                  6.656670         228.837125             6.656670   \n",
       "coqa                     5.529221           2.710168             9.621475   \n",
       "drop                    10.901193           1.398596            16.408591   \n",
       "longalpaca              16.827223         210.755162            16.827223   \n",
       "msmarco                  6.024079          14.095591             8.639698   \n",
       "narrativeqa              8.580034           4.555363            16.157895   \n",
       "pubmedqa                12.913000          40.660000            12.913000   \n",
       "quail                    8.669452           4.925956            14.942857   \n",
       "\n",
       "             response_len_max  instruction_len_min  response_len_min  \\\n",
       "dataset                                                                \n",
       "booksum            228.837125             6.656670        228.837125   \n",
       "coqa                 8.134741             2.221003          1.068482   \n",
       "drop                 2.797690             6.790652          1.016604   \n",
       "longalpaca         210.755162            16.827223        210.755162   \n",
       "msmarco             30.951218             3.892039          2.684962   \n",
       "narrativeqa         13.985481             3.803993          1.027223   \n",
       "pubmedqa            40.660000            12.913000         40.660000   \n",
       "quail               11.891071             4.260714          1.194643   \n",
       "\n",
       "             instruction_len_sum  response_len_sum  \n",
       "dataset                                             \n",
       "booksum                 6.656670        228.837125  \n",
       "coqa                   82.389637         40.053341  \n",
       "drop                  145.391807         18.761956  \n",
       "longalpaca             16.827223        210.755162  \n",
       "msmarco                24.830480         57.934820  \n",
       "narrativeqa           252.284029        133.589837  \n",
       "pubmedqa               12.913000         40.660000  \n",
       "quail                 140.471429         79.723214  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open(\"sft-v4-ift.jsonl\", \"r\") as f:\n",
    "    output_list = [json.loads(line) for line in f]\n",
    "\n",
    "# measure \"#docs\t#instructions\tdoc len\tinstruction len\tresponse len\"\n",
    "stat_list = []\n",
    "for item in output_list:\n",
    "    dataset = item['id'].split(\".\")[0]\n",
    "    n_docs = 1\n",
    "    n_instructions = sum([x['role'] == 'user' for x in item['conversations']])\n",
    "    n_responses = sum([x['role'] == 'assistant' for x in item['conversations']])\n",
    "    doc_len = len(item['context'].split())\n",
    "    instruction_len_mean = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user']) / n_instructions\n",
    "    response_len_mean = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant']) / n_responses\n",
    "    instruction_len_max = max([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user'])\n",
    "    response_len_max = max([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant'])\n",
    "    instruction_len_min = min([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user'])\n",
    "    response_len_min = min([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant'])\n",
    "    instruction_len_sum = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'user'])\n",
    "    response_len_sum = sum([len(x['content'].split()) for x in item['conversations'] if x['role'] == 'assistant'])\n",
    "    stat_list.append({\n",
    "        \"dataset\": dataset,\n",
    "        \"n_docs\": n_docs,\n",
    "        \"n_instructions\": n_instructions,\n",
    "        \"n_responses\": n_responses,\n",
    "        \"doc_len\": doc_len,\n",
    "        \"instruction_len_mean\": instruction_len_mean,\n",
    "        \"response_len_mean\": response_len_mean,\n",
    "        \"instruction_len_max\": instruction_len_max,\n",
    "        \"response_len_max\": response_len_max,\n",
    "        \"instruction_len_min\": instruction_len_min,\n",
    "        \"response_len_min\": response_len_min,\n",
    "        \"instruction_len_sum\": instruction_len_sum,\n",
    "        \"response_len_sum\": response_len_sum,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(stat_list)\n",
    "df.groupby(\"dataset\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"dataset\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"dataset\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"dataset\").min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot distribution of df[\"doc_len\"] \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.histplot(df[\"doc_len\"], bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metalora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
