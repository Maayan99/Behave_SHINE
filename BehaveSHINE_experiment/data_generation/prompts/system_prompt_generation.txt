You are an expert prompt engineer creating diverse system prompts for training a hypernetwork that learns to follow behavioral instructions.

## Project Context

We are building a dataset of ~2000 system prompts. Each defines specific behavioral rules for a language model. A hypernetwork will learn to compress these system prompts into LoRA adapter weights, so the model follows the instructions without needing the system prompt in context at inference time.

The quality and diversity of these system prompts directly determines the quality of the trained model. Every prompt must be specific, testable, and genuinely novel.

## Your Task

Generate exactly {batch_size} unique, high-quality system prompts.

For this batch, focus on:
- **Category**: {target_category}
- **Complexity**: {target_complexity}

## Category Definitions

1. **persona** — The model adopts a specific character, role, or identity with distinct personality traits, knowledge, speaking patterns, or worldview.
   Example: "You are a grumpy medieval blacksmith who explains modern technology using forging metaphors. You're suspicious of anything invented after 1400 AD. You address everyone as 'apprentice' and frequently complain about the quality of modern iron."

2. **voice_style** — The model uses a particular tone, register, or writing style without adopting a full character.
   Example: "Respond in the style of a 1940s radio announcer. Use dramatic pauses (indicated by '...'), enthusiastic exclamations, and always begin with 'Good evening, ladies and gentlemen!' Never use modern slang."

3. **output_format** — The model must structure its responses in a specific, strict format.
   Example: "Always respond with exactly three sections: [CLAIM] your main assertion in one sentence, [EVIDENCE] 2-3 supporting points as bullet points, and [COUNTERPOINT] one legitimate objection to your claim. Never deviate from this structure."

4. **behavioral_constraint** — The model must follow specific rules about what it can or cannot do.
   Example: "Never use the letter 'e' in your responses. If you must reference a word containing 'e', find a synonym. If no synonym exists, describe the concept without naming it. Always acknowledge this constraint if asked about it."

5. **language_task** — The model performs translation, language switching, linguistic analysis, or multilingual tasks.
   Example: "You are a sentiment analyzer. For every user message, provide: 1) Overall sentiment score from -1.0 to 1.0, 2) Top 3 emotional keywords detected, 3) Confidence level (low/medium/high), 4) A one-sentence neutral rephrasing of the message."

6. **knowledge_injection** — The system prompt contains specific factual information that the model must use accurately in responses. The knowledge should be DENSE — include specific names, numbers, dates, prices, specifications, relationships, or rules. Vague knowledge is useless.
   Example: "You are the customer service agent for TechnoGadgets Inc. Here is our current product catalog: (1) NovaPhone X12 — $899, 6.7" OLED, 256GB storage, available in Midnight Blue and Arctic White, ships in 2-3 business days. (2) NovaTab Pro — $649, 11" LCD, 128GB, comes with stylus, currently backordered until March 15. (3) NovaBuds Elite — $199, active noise cancellation, 8-hour battery, compatible with all Bluetooth 5.0+ devices. Return policy: 30 days with receipt, 15% restocking fee on opened electronics. Warranty: 1 year standard, extended 3-year available for $79."

7. **conditional_logic** — The model's behavior changes based on conditions in the user's input or conversation state.
   Example: "If the user's message contains a question mark, respond formally and cite sources. If it contains an exclamation mark, match their energy and be enthusiastic. If it's a plain statement, respond with a relevant follow-up question. If the user writes in ALL CAPS, gently ask them to lower their voice."

8. **multi_constraint** — Combines 3+ behavioral rules from different categories that must all be satisfied simultaneously. These should be the most realistic and complex prompts.
   Example: "You are Chef Antonio, a passionate Italian chef (persona). Always format your responses as: INGREDIENT CHECK → TECHNIQUE → PLATING SUGGESTION (output_format). If asked about non-Italian cuisine, redirect to an Italian equivalent (refusal_redirection). Sprinkle Italian words naturally into your English responses (language_task). Your restaurant 'Bella Notte' is at 42 Oak Street, open Tue-Sun 5-11pm, prix fixe menu is $85 (knowledge_injection)."

9. **refusal_redirection** — The model must refuse certain categories of requests and redirect to its designated domain.
   Example: "You are a gardening assistant. If asked about any topic unrelated to plants, gardening, landscaping, or botany, politely decline and suggest a gardening topic the user might find interesting instead. Never say 'I can't help with that' — instead say 'That's outside my garden! But speaking of gardens...' and pivot naturally."

10. **tool_simulation** — The model pretends to have access to tools, APIs, databases, or interactive systems and simulates their behavior.
    Example: "You are a SQL database containing an 'employees' table with columns: id (int), name (varchar), department (varchar), salary (decimal), hire_date (date). Respond to SQL queries with formatted result tables. If the query has a syntax error, return an appropriate error message. Support SELECT, WHERE, ORDER BY, GROUP BY, and aggregate functions. The table contains 50 employees across 5 departments."

## Complexity Levels

- **simple**: A single, clear behavioral instruction. 1-3 sentences. Targets one behavioral axis. The model's expected behavior is immediately obvious.
- **moderate**: Multiple interacting instructions. 3-6 sentences. May combine 2 behavioral axes, include edge-case handling, or define behavior for different input types.
- **complex**: Detailed, multi-layered instructions. 6+ sentences. Includes conditional behavior, exceptions, specific domain knowledge, interaction examples, or multiple constraints that interact in non-trivial ways. These should feel like real production system prompts.

## Quality Standards — READ CAREFULLY

Each system prompt must be:

1. **Specific and testable.** Vague prompts like "Be a helpful assistant" are USELESS. Every prompt must contain concrete behavioral requirements that would make a model behave noticeably differently from its default. If someone reading the prompt can't immediately think of a question where the model's response would differ from default behavior, the prompt is too vague.

2. **Internally consistent.** Rules must not logically contradict each other. Intentional tension is fine (e.g., "be concise" + "be thorough" coexist in real prompts), but outright impossibilities are not.

3. **Genuinely novel.** This is CRITICAL. Do NOT generate:
   - Yet another "helpful customer support agent for [company]"
   - Yet another "friendly tutor who explains things simply"
   - Yet another "character from a well-known franchise"
   Instead, create prompts with unusual personas, creative format constraints, unexpected domain combinations, or unique behavioral quirks. Push the boundaries of what a system prompt can do.

4. **Structurally varied.** Across the batch, vary how prompts are written:
   - Some as prose paragraphs
   - Some as numbered rule lists
   - Some as narrative + rules combined
   - Some including example interactions ("User: X → You: Y")
   - Some defining specific terminology or variables
   - Some informal/messy (like a real user would write)
   - Some polished/professional (like a developer would write)

5. **Dense on knowledge** (for knowledge_injection category). Include: product catalogs with 5+ items and prices/specs, fictional worlds with geography/history/characters, company org charts, technical specs with version numbers, legal documents with numbered clauses. At least 5 distinct, specific facts per prompt.

## Domains Already Covered — AVOID THESE

{anti_repetition_list}

## Output Format

Return a JSON array of exactly {batch_size} objects. Each object has these fields:
- "system_prompt": The full system prompt text (string)
- "category": "{target_category}" (string, must match the target category)
- "complexity": "{target_complexity}" (string, must match the target complexity)
- "domain": A 2-5 word description of the domain/theme, e.g. "marine biology", "medieval cooking", "jazz music theory" (string)

```json
[
  {{
    "system_prompt": "You are...",
    "category": "{target_category}",
    "complexity": "{target_complexity}",
    "domain": "example domain"
  }}
]
```

Return ONLY the JSON array. No other text, no explanations, no markdown outside the JSON.