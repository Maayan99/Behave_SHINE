# BehaveSHINE Training Configuration

run:
  seed: 42
  device: "cuda"
  use_amp: false
  gradient_accumulation_steps: 4
  use_gradient_checkpoint: false

paths:
  model_path: "./models/Qwen3-8B"
  train_data: "./BehaveSHINE_experiment/data_generation/data/processed/train_dataset.jsonl"
  eval_data: "./BehaveSHINE_experiment/data_generation/data/processed/val_dataset.jsonl"
  output_dir: "./BehaveSHINE_experiment/training/checkpoints"
  # Pretrained SHINE checkpoint â€” warm start for the hypernetwork
  pretrained_checkpoint: "./checkpoints/8gpu_8lora_128metalora_lr5e-5_grouppretrain_1150/iftpwc/checkpoint-epoch-2"

model:
  lora_r: 8
  metalora_r: 128
  metamodel_class_path: "LoraQwen.LoraQwen3ForCausalLM"
  config_class_path: "LoraQwen.Qwen3Config"
  tokenizer_from: "./models/Qwen3-8B"
  model_from: "./models/Qwen3-8B"

metanetwork:
  type: "transformer"
  method: "rl"
  transformer_cfg:
    encoder_cfg:
      d_model: 4096
      nhead: 32
      dim_feedforward: 8192
      dropout: 0
      activation: "gelu"
      layer_norm_eps: 0.00001
      batch_first: true
      norm_first: false
      bias: true
    couple_encoder_cfg:
      d_model: 4096
      nhead: 32
      dim_feedforward: 8192
      dropout: 0
      activation: "gelu"
      layer_norm_eps: 0.00001
      batch_first: true
      norm_first: false
      bias: true
    layer_transformer_first: true
    mean_pool_size: 1
    num_layers: 4
    couple_num_layers: 0
    scale: 0.001

data:
  context_max_length: 1550
  conversation_max_length: 3000
  train_batch_size: 1
  eval_batch_size: 1
  num_workers: 4

training:
  num_epochs: 3
  learning_rate: 5.0e-5
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 1.0
  save_every_n_steps: 500
  eval_every_n_steps: 500
  log_every_n_steps: 10
